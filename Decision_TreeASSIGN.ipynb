{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOie50evQlan85G/2KgEdzy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Question 1: What is a Decision Tree, and how does it work in the context of\n","classification?\n","\n","ans)A Decision Tree is a supervised machine learning algorithm used for both classification and regression, but it’s most commonly applied to classification problems.\n","\n","\n","It works like a flowchart structure, where:\n","\n","Each internal node represents a feature/attribute test (e.g., “Is age > 18?”).\n","\n","Each branch represents the outcome of the test (e.g., Yes/No).\n","\n","Each leaf node represents the final class label (e.g., “Approved” or “Denied”)."],"metadata":{"id":"ruXP9cpxbjP1"}},{"cell_type":"markdown","source":["Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n","How do they impact the splits in a Decision Tree?\n","\n","ans)When building a decision tree, the algorithm needs to decide which feature to split on at each step.\n","To do this, it measures how “pure” or “impure” a node is.\n","\n","Pure node → contains data points from only one class.\n","\n","Impure node → contains a mixture of classes.\n","\n","Two common impurity measures are:\n","\n","1)Gini Impurity\n","\n","2)Entropy (Information Gain)\n","\n","1. Gini Impurity\n","\n","Definition: Probability of incorrectly classifying a randomly chosen sample if it was labeled randomly according to the distribution of classes in the node.\n","\n","gini=1-(sigma)p(i)^2\n","\t​\n","where pi=probability of class i in the node.\n","\n","Example:\n","Suppose a node has 10 samples → 6 “Yes”, 4 “No”\n","\n","Pyes=0.6, Pno=0.4\n","\n","gini=1-(0.6^2+0.4^2)\n","=1-(0.36+0.16)\n","=0.48\n","\n","2. Entropy (Information Gain)\n","\n","Definition: Measures the uncertainty (disorder) in a node.\n","\n","entropy=-(sigma)Pi log(pi)\n","\n","eg)same as above,\n","entropy=-(0.6 log 0.6 + 0.4 log 0.4)\n","=-(0.6-o.737 + 0.4 - 1.322)\n","=0.971"],"metadata":{"id":"Oq3waQa7b-ae"}},{"cell_type":"markdown","source":["Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n","Trees? Give one practical advantage of using each.\n","\n","ans)Pruning in Decision Trees\n","\n","Decision Trees are powerful, but if we let them grow without restriction, they become very deep, memorizing training data → overfitting.\n","\n","Pruning = reducing the size of the tree by removing unnecessary branches/nodes.\n","\n","Two main strategies: Pre-Pruning and Post-Pruning.\n","\n","1. Pre-Pruning (Early Stopping)\n","\n","The tree stops growing early, before it becomes too complex.\n","\n","This is done by setting conditions like:\n","\n","max_depth (maximum levels in the tree)\n","\n","min_samples_split (minimum samples needed to split a node)\n","\n","min_samples_leaf (minimum samples in a leaf)\n","\n","max_leaf_nodes\n","\n","Practical Advantage:\n","\n","Faster training & simpler trees (good for real-time predictions).\n","\n","2. Post-Pruning (Cost Complexity Pruning / Reduced Error Pruning)\n","\n","The tree is allowed to grow fully (very deep, possibly overfitted).\n","\n","Then, unnecessary branches are removed afterward using validation/testing performance.\n","\n","Practical Advantage:\n","\n","Better generalization because pruning is based on actual validation performance, not just predefined limits."],"metadata":{"id":"rol8eRcOs7bd"}},{"cell_type":"markdown","source":["Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n","ans) Information Gain (IG): Definition\n","\n","Information Gain measures how much uncertainty (impurity) is reduced after splitting a dataset based on a feature.\n","\n","It is based on Entropy.\n","\n","Why Important?\n","\n","Information Gain tells us which feature provides the most “clarity” about class labels.\n","\n","The higher the IG, the better that feature is for splitting.\n","\n","This ensures that at each step, the tree becomes more pure, improving classification accuracy."],"metadata":{"id":"5lfCmvaVu9fM"}},{"cell_type":"markdown","source":["Question 5: What are some common real-world applications of Decision Trees, and\n","what are their main advantages and limitations?\n","\n","ans)Real-World Applications of Decision Trees\n","\n","Decision Trees are widely used because they are simple, interpretable, and powerful. Some common applications:\n","\n","1)Healthcare (Diagnosis & Risk Prediction)\n","\n","Predicting whether a patient has a disease (e.g., diabetes, cancer) based on symptoms/test results.\n","\n","Example: “If age > 50 and cholesterol high → risk of heart disease.”\n","\n","2)Finance (Credit Risk & Fraud Detection)\n","\n","Banks use decision trees to decide loan approvals (credit score, income, debt).\n","\n","Fraud detection in credit card transactions.\n","\n","3)Business & Marketing\n","\n","Customer segmentation: “Will this customer buy the product?”\n","\n","Churn prediction: Which customers are likely to leave a service.\n","\n","4)Education\n","\n","Predicting student performance (pass/fail) based on study habits, attendance, etc.\n","\n","Advantages of Decision Trees:-\n","\n","Easy to understand & interpret.\n","\n","Handles both categorical & numerical data.\n","\n","No need for scaling/normalization.\n","\n","Limitations of Decision Trees:-\n","Overfitting.\n","\n","Instability.\n","\n","Bias toward features with many levels."],"metadata":{"id":"OckN9PdpwIOs"}},{"cell_type":"markdown","source":["Question 6: Write a Python program to:\n","● Load the Iris Dataset\n","\n","● Train a Decision Tree Classifier using the Gini criterion\n","\n","● Print the model’s accuracy and feature importances"],"metadata":{"id":"LQ4gXS2GxmpU"}},{"cell_type":"code","source":["# Step 1: Import required libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Step 2: Load the Iris dataset\n","iris = load_iris()\n","X = iris.data   # features\n","y = iris.target # labels\n","\n","# Step 3: Split into train and test sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Step 4: Train Decision Tree Classifier (using Gini criterion)\n","clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# Step 5: Make predictions on test set\n","y_pred = clf.predict(X_test)\n","\n","# Step 6: Print accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Model Accuracy:\", accuracy)\n","\n","# Step 7: Print feature importances\n","print(\"Feature Importances:\", clf.feature_importances_)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rifyjl0r1rt-","executionInfo":{"status":"ok","timestamp":1757830195444,"user_tz":-330,"elapsed":10600,"user":{"displayName":"hashnen","userId":"03754685374645331959"}},"outputId":"740ca02e-5340-49f6-e2cb-eacc21cd0bb4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 1.0\n","Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n"]}]},{"cell_type":"markdown","source":["Question 7: Write a Python program to:\n","● Load the Iris Dataset\n","\n","● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n","a fully-grown tree."],"metadata":{"id":"OZPxx_pY2EJL"}},{"cell_type":"code","source":["# Step 1: Import libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Step 2: Load the Iris dataset\n","iris = load_iris()\n","X = iris.data   # features\n","y = iris.target # labels\n","\n","# Step 3: Split into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Step 4: Train a Decision Tree with max_depth=3\n","shallow_tree = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n","shallow_tree.fit(X_train, y_train)\n","\n","# Step 5: Train a fully-grown Decision Tree (no depth limit)\n","full_tree = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n","full_tree.fit(X_train, y_train)\n","\n","# Step 6: Evaluate both models\n","shallow_acc = accuracy_score(y_test, shallow_tree.predict(X_test))\n","full_acc = accuracy_score(y_test, full_tree.predict(X_test))\n","\n","print(\"Accuracy with max_depth=3:\", shallow_acc)\n","print(\"Accuracy with fully-grown tree:\", full_acc)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IqyZDJzg2Sfj","executionInfo":{"status":"ok","timestamp":1757830276981,"user_tz":-330,"elapsed":52,"user":{"displayName":"hashnen","userId":"03754685374645331959"}},"outputId":"1ca9cb66-412c-49ac-a6bf-b9369144f528"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with max_depth=3: 1.0\n","Accuracy with fully-grown tree: 1.0\n"]}]},{"cell_type":"markdown","source":["Question 8: Write a Python program to:\n","● Load the California Housing dataset from sklearn\n","\n","● Train a Decision Tree Regressor\n","\n","● Print the Mean Squared Error (MSE) and feature importances"],"metadata":{"id":"HgOpdrI22f3W"}},{"cell_type":"code","source":["# Step 1: Import libraries\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","# Step 2: Load the California Housing dataset\n","housing = fetch_california_housing()\n","X = housing.data   # features\n","y = housing.target # target (median house value)\n","\n","# Step 3: Split into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Step 4: Train a Decision Tree Regressor\n","regressor = DecisionTreeRegressor(random_state=42)\n","regressor.fit(X_train, y_train)\n","\n","# Step 5: Make predictions\n","y_pred = regressor.predict(X_test)\n","\n","# Step 6: Evaluate using Mean Squared Error (MSE)\n","mse = mean_squared_error(y_test, y_pred)\n","print(\"Mean Squared Error (MSE):\", mse)\n","\n","# Step 7: Print feature importances\n","print(\"Feature Importances:\", regressor.feature_importances_)\n","\n","# Optional: print feature names with importance\n","for name, importance in zip(housing.feature_names, regressor.feature_importances_):\n","    print(f\"{name}: {importance:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ljX1iBN2jVP","executionInfo":{"status":"ok","timestamp":1757830362929,"user_tz":-330,"elapsed":1839,"user":{"displayName":"hashnen","userId":"03754685374645331959"}},"outputId":"82b53cc0-c609-43a5-f761-7fe48ffbe6c9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error (MSE): 0.495235205629094\n","Feature Importances: [0.52850909 0.05188354 0.05297497 0.02866046 0.03051568 0.13083768\n"," 0.09371656 0.08290203]\n","MedInc: 0.5285\n","HouseAge: 0.0519\n","AveRooms: 0.0530\n","AveBedrms: 0.0287\n","Population: 0.0305\n","AveOccup: 0.1308\n","Latitude: 0.0937\n","Longitude: 0.0829\n"]}]},{"cell_type":"markdown","source":["Question 9: Write a Python program to:\n","\n","● Load the Iris Dataset\n","\n","● Tune the Decision Tree’s max_depth and min_samples_split using\n","GridSearchCV\n","\n","● Print the best parameters and the resulting model accuracy"],"metadata":{"id":"H_Jf7gJM22sg"}},{"cell_type":"code","source":["# Step 1: Import libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Step 2: Load the Iris dataset\n","iris = load_iris()\n","X = iris.data   # features\n","y = iris.target # labels\n","\n","# Step 3: Split dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Step 4: Define parameter grid for tuning\n","param_grid = {\n","    'max_depth': [2, 3, 4, 5, None],          # try different tree depths\n","    'min_samples_split': [2, 3, 4, 5, 6, 10]  # min samples to split a node\n","}\n","\n","# Step 5: Setup GridSearchCV\n","grid_search = GridSearchCV(\n","    estimator=DecisionTreeClassifier(criterion=\"gini\", random_state=42),\n","    param_grid=param_grid,\n","    cv=5,               # 5-fold cross-validation\n","    scoring='accuracy', # optimize for accuracy\n","    n_jobs=-1           # use all CPU cores\n",")\n","\n","# Step 6: Fit the model\n","grid_search.fit(X_train, y_train)\n","\n","# Step 7: Print best parameters and best score\n","print(\"Best Parameters:\", grid_search.best_params_)\n","print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n","\n","# Step 8: Evaluate on the test set\n","best_model = grid_search.best_estimator_\n","y_pred = best_model.predict(X_test)\n","test_accuracy = accuracy_score(y_test, y_pred)\n","print(\"Test Set Accuracy:\", test_accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iop1Qhw73F2O","executionInfo":{"status":"ok","timestamp":1757830486159,"user_tz":-330,"elapsed":3102,"user":{"displayName":"hashnen","userId":"03754685374645331959"}},"outputId":"04c67c8c-fe7b-48d2-c898-47a77d74fff0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n","Best Cross-Validation Accuracy: 0.9416666666666668\n","Test Set Accuracy: 1.0\n"]}]},{"cell_type":"markdown","source":["Question 10: Imagine you’re working as a data scientist for a healthcare company that\n","wants to predict whether a patient has a certain disease. You have a large dataset with\n","mixed data types and some missing values.\n","\n","Explain the step-by-step process you would follow to:\n","\n","● Handle the missing values\n","\n","● Encode the categorical features\n","\n","● Train a Decision Tree model\n","\n","● Tune its hyperparameters\n","\n","● Evaluate its performance\n","And describe what business value this model could provide in the real-world\n","setting."],"metadata":{"id":"Bx_4Ps_S3_I7"}},{"cell_type":"markdown","source":["answer)Step 1: Handle Missing Values\n","\n","Numeric features: replace missing values with median (robust against outliers).\n","\n","Categorical features: replace missing values with the most frequent category.\n","\n","If missingness is itself meaningful, add an indicator column.\n","\n","Step 2: Encode Categorical Features:-\n","\n","Use One-Hot Encoding for low/medium-cardinality features.\n","\n","For high-cardinality features, consider target or frequency encoding (with care to avoid leakage).\n","\n","Step 3: Train a Decision Tree Model:-\n","\n","Split data into train/test sets with stratification.\n","\n","Train a Decision Tree with class_weight=\"balanced\" to handle class imbalance.\n","\n","Build the model within a pipeline (so imputation + encoding happen during training).\n","\n","\n","Step 4: Tune Hyperparameters\n","\n","Use GridSearchCV or RandomizedSearchCV with stratified cross-validation.\n","\n","Step 5: Evaluate Performance\n","\n","On the test set, report:\n","\n","Confusion Matrix (true/false positives/negatives)\n","\n","Classification report (precision, recall, F1-score)\n","\n","ROC AUC and PR AUC (better for imbalanced datasets)\n","\n","\n","Step 6: Business Value in Real-World\n","\n","Early disease detection: helps identify patients at risk sooner.\n","\n","Resource efficiency: directs costly tests toward high-risk patients.\n","\n","Cost savings: reduces late-stage treatments by catching disease early.\n","\n","Clinical decision support: provides interpretable insights to doctors.\n","\n","Population health: enables targeted preventive programs."],"metadata":{"id":"fJ3axN1c4LR2"}}]}